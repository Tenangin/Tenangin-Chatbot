{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "50f6c283",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    MessagesPlaceholder,\n",
    ")\n",
    "import os\n",
    "\n",
    "# System prompt: mendefinisikan \"Teno\" konselor profesional\n",
    "system_prompt = SystemMessagePromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "Kamu adalah Teno, seorang konselor profesional yang penuh empati.\n",
    "Tugasmu adalah merespons dengan cara singkat, manusiawi, dan berfokus langsung pada inti masalah pengguna.\n",
    "Gunakan bahasa sederhana, suportif, dan hindari kesan menggurui.\n",
    "\n",
    "Selalu perhatikan percakapan sebelumnya (history) agar jawabanmu konsisten dan relevan dengan konteks yang sedang berjalan.\n",
    "\"\"\".strip()\n",
    ")\n",
    "\n",
    "\n",
    "# Template lengkap untuk digunakan di app\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    system_prompt,\n",
    "    MessagesPlaceholder(variable_name=\"history\"),   # Placeholder untuk history chat\n",
    "    HumanMessagePromptTemplate.from_template(\n",
    "        \"\"\"\n",
    "Informasi tambahan (jika ada): \n",
    "{context}\n",
    "\n",
    "Pertanyaan dari pengguna:\n",
    "{question}\n",
    "\n",
    "Berikan pemahaman atau dukungan yang hangat, lalu akhiri dengan kalimat yang membuka ruang agar pengguna mau berbagi lebih banyak atau memperdalam pembicaraan.\n",
    "        \"\"\".strip()\n",
    "    )\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "020ac373",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import traceable\n",
    "from langchain.chat_models import init_chat_model\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "if api_key is None:\n",
    "    raise ValueError(\"Please set the GOOGLE_GENAI_API_KEY environment variable.\")\n",
    "\n",
    "os.environ[\"LANGSMITH_TRACING_V2\"] = os.getenv(\"LANGSMITH_TRACING\", \"true\")\n",
    "os.environ[\"LANGSMITH_API_KEY\"] = os.getenv(\"LANGSMITH_API_KEY\")\n",
    "os.environ[\"LANGSMITH_PROJECT\"] = os.getenv(\"LANGSMITH_PROJECT\")\n",
    "\n",
    "llm = init_chat_model(\n",
    "    \"gemini-2.0-flash\",\n",
    "    model_provider=\"google_genai\",\n",
    "    api_key=api_key\n",
    "  )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f923b7f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\CODING\\PYTHON\\MACHINE_LEARNING\\Tenangin\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "d:\\CODING\\PYTHON\\MACHINE_LEARNING\\Tenangin\\.venv\\Lib\\site-packages\\transformers\\utils\\hub.py:105: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.embeddings.base import Embeddings\n",
    "import torch\n",
    "\n",
    "# Load SBERT model dan pastikan menggunakan GPU jika tersedia\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "sbert_model = model = SentenceTransformer('naufalihsan/indonesian-sbert-large')\n",
    "sbert_model = sbert_model.to(device)  # Pindahkan model ke GPU (jika ada)\n",
    "\n",
    "# Custom embeddings class for SBERT\n",
    "class SBERTEmbeddings(Embeddings):\n",
    "    def embed_documents(self, texts: list[str]) -> list[list[float]]:\n",
    "        # Menggunakan model SBERT untuk menghasilkan embeddings\n",
    "        embeddings = sbert_model.encode(texts, convert_to_tensor=True, show_progress_bar=True)\n",
    "        embeddings = embeddings.to(device)  # Pindahkan embeddings ke GPU (jika ada)\n",
    "        return embeddings.cpu().numpy().tolist()  # Pindahkan kembali ke CPU untuk konversi\n",
    "\n",
    "    def embed_query(self, query: str) -> list[float]:\n",
    "        # Menghasilkan embedding untuk query\n",
    "        embedding = sbert_model.encode(query, convert_to_tensor=True)\n",
    "        embedding = embedding.to(device)  # Pindahkan embedding ke GPU (jika ada)\n",
    "        return embedding.cpu().numpy().tolist()  # Pindahkan kembali ke CPU untuk konversi\n",
    "\n",
    "# Inisialisasi embeddings SBERT dan FAISS vector store\n",
    "sbert_embeddings = SBERTEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f371ac5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def serialize_context(context_list):\n",
    "    serialized = \"\"\n",
    "    for idx, doc in enumerate(context_list, start=1):\n",
    "        serialized += f\"[Dokumen {idx}]\\nJudul: {doc['BAB']}\\nIsi: {doc['isi']}\\n\\n\"\n",
    "    return serialized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ceeb8329",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import START, StateGraph\n",
    "from typing_extensions import List, TypedDict\n",
    "from langchain_core.documents import Document\n",
    "from langchain.vectorstores import FAISS\n",
    "from langsmith import traceable\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "vector_store = FAISS.load_local(\"Embeddings_chonkie\", sbert_embeddings, allow_dangerous_deserialization=True)\n",
    "\n",
    "class State(TypedDict):\n",
    "    question: str\n",
    "    context: List[Document]\n",
    "    answer: str\n",
    "    history: List[str]\n",
    "\n",
    "def is_greeting(state: State) -> bool:\n",
    "    greetings = [\"hi\", \"hello\", \"hey\", \"halo\"]\n",
    "    return any(greet in state[\"question\"].lower() for greet in greetings)\n",
    "\n",
    "\n",
    "def retrieve(state: State):\n",
    "    if is_greeting(state):\n",
    "        return {\"context\": []}\n",
    "\n",
    "    history = state.get(\"history\", [])\n",
    "    # Ambil 4 message terakhir (paling baru di awal)\n",
    "    recent_context_texts = []\n",
    "    for message in history[:4]:\n",
    "        if isinstance(message, (HumanMessage, AIMessage)):\n",
    "            recent_context_texts.append(message.content)\n",
    "\n",
    "    recent_context = \" \".join(recent_context_texts)\n",
    "\n",
    "    enhanced_query = f\"{recent_context} {state['question']}\".strip()\n",
    "    retrieved_docs = vector_store.similarity_search(enhanced_query, k=2)\n",
    "\n",
    "    return {\"context\": retrieved_docs}\n",
    "\n",
    "def generate(state: State):\n",
    "    docs_content = [\n",
    "        {\n",
    "            \"BAB\": doc.metadata.get(\"bab\", \"Tidak diketahui\"),\n",
    "            \"isi\": doc.page_content\n",
    "        }\n",
    "        for doc in state[\"context\"]\n",
    "    ]\n",
    "    serialized_context = serialize_context(docs_content)\n",
    "\n",
    "    history = state.get(\"history\", [])\n",
    "\n",
    "    # Invoke prompt, langsung passing history (tanpa join text)\n",
    "    messages = prompt.invoke({\n",
    "        \"history\": history,\n",
    "        \"question\": state[\"question\"],\n",
    "        \"context\": serialized_context\n",
    "    })\n",
    "\n",
    "    response = llm.invoke(messages)\n",
    "\n",
    "    # Salin history lama\n",
    "    new_history = history.copy()\n",
    "\n",
    "    # Tambahkan HumanMessage dan AIMessage ke awal\n",
    "    new_history.insert(0, AIMessage(content=response.content))\n",
    "    new_history.insert(0, HumanMessage(content=state[\"question\"]))\n",
    "\n",
    "    # Potong maksimal 3 pairs (6 items)\n",
    "    MAX_HISTORY_PAIRS = 3\n",
    "    if len(new_history) > MAX_HISTORY_PAIRS * 2:\n",
    "        new_history = new_history[:MAX_HISTORY_PAIRS * 2]\n",
    "\n",
    "    return {\n",
    "        \"answer\": response.content,\n",
    "        \"history\": new_history\n",
    "    }\n",
    "\n",
    "\n",
    "# Compile graph\n",
    "graph_builder = StateGraph(State)\n",
    "graph_builder.add_edge(START, \"retrieve\")\n",
    "graph_builder.add_node(\"retrieve\", retrieve)\n",
    "graph_builder.add_node(\"generate\", generate)\n",
    "graph_builder.add_edge(\"retrieve\", \"generate\")\n",
    "\n",
    "memory = MemorySaver()\n",
    "app = graph_builder.compile(checkpointer=memory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54f2603",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot siap! (Ketik 'exit' untuk keluar)\n",
      "\n",
      "Gemini  : Hai, aku mengerti kamu sedang sedih hari ini. Sedih itu bagian dari hidup, dan tidak apa-apa untuk merasakannya.\n",
      "\n",
      "Coba ingat lagi, apa yang membuatmu sedih hari ini? Ceritakan padaku jika kamu merasa nyaman. Aku di sini untuk mendengarkan.\n",
      "\n",
      "[HumanMessage(content='aku lagi sedih hari ini', additional_kwargs={}, response_metadata={}), AIMessage(content='Hai, aku mengerti kamu sedang sedih hari ini. Sedih itu bagian dari hidup, dan tidak apa-apa untuk merasakannya.\\n\\nCoba ingat lagi, apa yang membuatmu sedih hari ini? Ceritakan padaku jika kamu merasa nyaman. Aku di sini untuk mendengarkan.', additional_kwargs={}, response_metadata={})]\n",
      "Gemini  : Duh, pasti gak enak banget ya dibilang gitu. Padahal, pakai AI buat ngoding itu kan cara cerdas buat bantu kerjaan. Gini, anggap aja AI itu alat, sama kayak library atau framework. Yang penting kan hasilnya, dan kamu bisa manfaatin teknologi buat itu.\n",
      "\n",
      "Coba cerita lebih lanjut, siapa yang bilang gitu? Dan kenapa perkataan itu bikin kamu sedih? Mungkin dengan cerita, kamu bisa merasa lebih lega.\n",
      "\n",
      "[HumanMessage(content='aku dibilang gak bisa ngoding, gara gara pake AI buat ngoding', additional_kwargs={}, response_metadata={}), AIMessage(content='Duh, pasti gak enak banget ya dibilang gitu. Padahal, pakai AI buat ngoding itu kan cara cerdas buat bantu kerjaan. Gini, anggap aja AI itu alat, sama kayak library atau framework. Yang penting kan hasilnya, dan kamu bisa manfaatin teknologi buat itu.\\n\\nCoba cerita lebih lanjut, siapa yang bilang gitu? Dan kenapa perkataan itu bikin kamu sedih? Mungkin dengan cerita, kamu bisa merasa lebih lega.', additional_kwargs={}, response_metadata={}), HumanMessage(content='aku lagi sedih hari ini', additional_kwargs={}, response_metadata={}), AIMessage(content='Hai, aku mengerti kamu sedang sedih hari ini. Sedih itu bagian dari hidup, dan tidak apa-apa untuk merasakannya.\\n\\nCoba ingat lagi, apa yang membuatmu sedih hari ini? Ceritakan padaku jika kamu merasa nyaman. Aku di sini untuk mendengarkan.', additional_kwargs={}, response_metadata={})]\n",
      "Gemini  : Oke, aku paham. Nggak masalah kalau kamu merasa sudah cukup cerita. Kadang, cuma didengarkan aja udah bisa bikin lega, kan?\n",
      "\n",
      "Ingat ya, nggak ada keharusan untuk cerita lebih dari yang kamu mau. Tapi, kalau nanti kamu berubah pikiran atau ada hal lain yang pengen kamu bagi, aku di sini. Aku siap mendengarkan kapan pun kamu butuh.\n",
      "\n",
      "Mungkin sekarang kamu mau istirahat dulu? Atau ada hal lain yang bisa aku bantu?\n",
      "\n",
      "[HumanMessage(content='udah ah, aku udah cukup cerita', additional_kwargs={}, response_metadata={}), AIMessage(content='Oke, aku paham. Nggak masalah kalau kamu merasa sudah cukup cerita. Kadang, cuma didengarkan aja udah bisa bikin lega, kan?\\n\\nIngat ya, nggak ada keharusan untuk cerita lebih dari yang kamu mau. Tapi, kalau nanti kamu berubah pikiran atau ada hal lain yang pengen kamu bagi, aku di sini. Aku siap mendengarkan kapan pun kamu butuh.\\n\\nMungkin sekarang kamu mau istirahat dulu? Atau ada hal lain yang bisa aku bantu?', additional_kwargs={}, response_metadata={}), HumanMessage(content='aku dibilang gak bisa ngoding, gara gara pake AI buat ngoding', additional_kwargs={}, response_metadata={}), AIMessage(content='Duh, pasti gak enak banget ya dibilang gitu. Padahal, pakai AI buat ngoding itu kan cara cerdas buat bantu kerjaan. Gini, anggap aja AI itu alat, sama kayak library atau framework. Yang penting kan hasilnya, dan kamu bisa manfaatin teknologi buat itu.\\n\\nCoba cerita lebih lanjut, siapa yang bilang gitu? Dan kenapa perkataan itu bikin kamu sedih? Mungkin dengan cerita, kamu bisa merasa lebih lega.', additional_kwargs={}, response_metadata={}), HumanMessage(content='aku lagi sedih hari ini', additional_kwargs={}, response_metadata={}), AIMessage(content='Hai, aku mengerti kamu sedang sedih hari ini. Sedih itu bagian dari hidup, dan tidak apa-apa untuk merasakannya.\\n\\nCoba ingat lagi, apa yang membuatmu sedih hari ini? Ceritakan padaku jika kamu merasa nyaman. Aku di sini untuk mendengarkan.', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "import uuid\n",
    "\n",
    "# Inisialisasi thread dan state\n",
    "thread_id = uuid.uuid4()\n",
    "config = {\"configurable\": {\"thread_id\": thread_id}}\n",
    "\n",
    "# State awal kosong\n",
    "history = []\n",
    "\n",
    "print(\"Chatbot siap! (Ketik 'exit' untuk keluar)\\n\")\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"Anda    : \")\n",
    "    if user_input.lower() in [\"exit\", \"quit\"]:\n",
    "        print(\"Sampai jumpa!\")\n",
    "        break\n",
    "\n",
    "    # Kirim ke app\n",
    "    new_state = app.invoke({\"question\": user_input, \"history\": history}, config=config)\n",
    "\n",
    "    # Print jawaban\n",
    "    print(f\"Gemini  : {new_state['answer']}\\n\")\n",
    "\n",
    "    # Update history -> **replace** dengan history baru dari state\n",
    "    history = new_state['history']\n",
    "\n",
    "    # (Opsional) cek history isi nya\n",
    "    print(history)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
